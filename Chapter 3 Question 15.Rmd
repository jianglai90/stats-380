---
title: "PM Take Home Exam"
author: "Lai Jiang"
date: "7/18/2020"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Chapter 2 question 10
##10(a)
```{r}
rm(list=ls())
library(MASS)
Boston
?Boston
```
##The Boston data frame has 506 rows and 14 columns.

##2(b)
```{r}
all_correlations <- cor(Boston)
print(all_correlations[, 14])
attach(Boston)
```
```{r}
cols <- c(14, 13, 6, 11, 3, 10)
pairs(Boston[, cols])
```
## It is difficult to discern relationships for most of the variables compare to medv. Lstat and rm shows some promising relationship based on graphical observation

##2(d)

```{r}
summary(crim)
summary(tax)
summary(ptratio)

```
##if we use crim >40 as an indicator of high crime rate.
##Area in 381, 419,406,411,415,405 has very high crime rate
##Area in 491,490,489,493,492 has very high tax rate of 711 per 10k property value
##Area in 356,355 has the highest Pupil-teacher ratio of 22.

##2(e)
```{r}
table(chas)
```
##About 35 areas are bound to the Charles river

##2(f)
```{r}
summary(ptratio)
```
##The median ptratio is 19.05

##2(g)

```{r}
which(medv == min(medv))
```

## Area 399 and 406 has the lowest medv
##These areas typically have close to max measured lstat, percentage of black population, high pt ratio, high tax rate,  high accessibility to highway(maybe indicating they are near downtown), close to min value of weighted mean of distances to 5 Boston employment centers, oldest homes,  rms per house in 1st quartile(small housing area),  high pollution level,  high proportion of non-retail business, no proportion of residential land zones for lots over 25000 sq ft. high crime rate above 3rd quartile

##2(h)
```{r}

which(rm>7)
```
```{r}
which(rm>8)
```
## Area with 8 rooms tend to have low lstat,  variable ptratio, tax rate, average distance to employment, low pollution level,  and  relative low crime rate. Some even have residential land zoned for lots over 25,000 sqft.

## Chapter 3 question 15
#a
```{r}
library(MASS)
View(Boston)
attach(Boston)
preds<-names(Boston)[-1]
mods<-list()
stats<-list()
for (i in seq(along=preds)){
  p<-preds[i]
  mods[[i]] <- lm(as.formula(paste0("crim ~ ",p)),data=Boston)
}
stats<-t(sapply(mods, function(x) coef(summary(x))[2,]))
r_squared<-sapply(mods,function(x) summary(x)$r.squared)
stats<-cbind(stats,r_squared)
rownames(stats) <-preds
stats2<-stats[order(abs(stats[,"t value"]),decreasing=TRUE),]
stats2
plot(lm(crim~zn))
plot(lm(crim~nox))
plot(lm(crim~black))
plot(lm(crim~medv))



# most demonstrate a statisticall signifcant relationship with crim, except chas
```
```{r}
#b
limmulti=lm(crim~.,Boston)
summary(limmulti)
#I choose p=0.005, thus we reject null hypothesis H0 : Î²j = 0 for dis, rad, mdev

```


```{r}
plot(stats[,1],coef(limmulti)[-1])
cbind(stats[,1],coef(limmulti)[-1])
#simple linear regression gave much larger coef for most of variables compare to multi linear regression. some even flip to positive or negative numbers.
```

```{r}

lm.fit1=lm(crim~poly(zn,3))
summary(lm.fit1)
lm.fit2=lm(crim~poly(indus,3))
summary(lm.fit2)
lm.fit3=lm(crim~poly(nox,3))
summary(lm.fit3)
lm.fit4=lm(crim~poly(rm,3))
summary(lm.fit4)
lm.fit5=lm(crim~poly(age,3))
summary(lm.fit5)
lm.fit6=lm(crim~poly(dis,3))
summary(lm.fit6)
lm.fit7=lm(crim~poly(rad,3))
summary(lm.fit7)
lm.fit8=lm(crim~poly(tax,3))
summary(lm.fit8)
lm.fit9=lm(crim~poly(ptratio,3))
summary(lm.fit9)
lm.fit10=lm(crim~poly(black,3))
summary(lm.fit10)
lm.fit11=lm(crim~poly(lstat,3))
summary(lm.fit11)
lm.fit12=lm(crim~poly(medv,3))
summary(lm.fit12)
#Yes, there is evidence of non-linear association for many of the predictors. crim vs dis, crim, medv
```
```{r}
## Chapter 6 question 9
rm(list=ls())
data(College,package="ISLR")
n<-nrow(College)
```


```{r}
#9(a)
set.seed(1)
train <- sample(1:n, n/2) #split the sample and testing into half and half
test <- (-train)
y.test <- College$Apps[test]
y.train <- College$Apps[train]
ref.err<-mean((mean(y.test)-y.test)^2)
ref.err
```

```{r}
#9(b)
#fit a linear model using least square on the training set and report the test error obtained
lm.mod <- lm(Apps ~ ., data=College[train,])
lm.pred <- predict(lm.mod, College[test,])
lm.err<-mean((lm.pred-y.test)^2)
lm.err
```
```{r}
#9(c)
#fit a ridge regression model on the training set with lambda chosen by cross validation 
set.seed(1)
library(glmnet)
x.mod <- model.matrix(Apps ~ ., data=College)[,-1]
ridge.mod.cv<-cv.glmnet(x.mod[train,],y.train,alpha=0)
ridge.best.lam <- ridge.mod.cv$lambda.min
ridge.mod <- glmnet(x.mod[train,],y.train,alpha=0)
ridge.pred <- predict(ridge.mod,newx=x.mod[test,],s=ridge.best.lam)
ridge.err<-mean((ridge.pred-y.test)^2)
ridge.err
```
```{r}
#9(d)
#fit a lasso model on the training set, with lambda chosen by cross validation.
set.seed(1)
x.mod <- model.matrix(Apps ~ ., data=College)[,-1]
lasso.mod.cv <- cv.glmnet(x.mod[train,],y.train,alpha=1)
lasso.best.lam <- lasso.mod.cv$lambda.min
lasso.mod <- glmnet(x.mod[train,],y.train,alpha=1)
lasso.pred <- predict(lasso.mod,newx=x.mod[test,],s=lasso.best.lam)
(lasso.err<-mean((lasso.pred-y.test)^2))
```
```{r}
##9(e) fit a PCR model on training set, with M chosen by cross validation
library(pls)
set.seed(1)
pcr.mod <- pcr(Apps ~ ., data=College, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.mod,val.type="MSEP")
```


```{r}
summary(pcr.mod)
```
```{r}
pcr.pred <- predict(pcr.mod,College[test,],ncomp=16)
pcr.err<-mean((pcr.pred-y.test)^2)
pcr.err
#17 components considered
```

```{r}
##9(f)
library(pls)
set.seed(1)
pls.mod <- plsr(Apps ~ ., data=College, subset=train, validation="CV")
validationplot(pls.mod, val.type="MSEP")
#roughly the same CV after 4 components
```
```{r}
summary(pls.mod)
pls.pred <- predict(pls.mod,College[test,],ncomp=4)
pls.err<-mean((pls.pred-y.test)^2)
```
```{r}
#9(g)
rbind(ref.err,lm.err,ridge.err,lasso.err,pcr.err,pls.err)
#as we can see lasso nd ridge performed better than PCR and PLS, linear model  performed better than PCR and PLS
#Ridge was the best
```

## Chapter 6 question 11
```{r}
rm(list=ls())
data(Boston,package="MASS")
head(Boston)
Boston.mat <- model.matrix(crim ~ ., Boston)
```

## split the set into test and training
```{r}
set.seed(1)
train<-sample.int(nrow(Boston),size=nrow(Boston)*.5)
test<--train
y.train <- Boston$crim[train]
y.test <- Boston$crim[test]
```

## linear models, use best subsets to select the best train set using BIC
## then get the MSE on the test set

```{r}
require(leaps)
full.fit <- regsubsets(crim ~ ., data=Boston[train,],nvmax=ncol(Boston))
lm.best.coef <- coef(full.fit,id=which.min(summary(full.fit)$bic))
lm.pred <- Boston.mat[test,names(lm.best.coef)]%*%lm.best.coef
lm.err <- mean((lm.pred-y.test)^2)
lm.err
```

```{r}
full.lm.mod <- lm(crim ~ ., Boston, subset=train)
lm.full.pred <- predict(full.lm.mod, Boston[test,])
lm.full.err <- mean((lm.full.pred - y.test)^2)
lm.full.err
```
## Ridge and Lasso using CV
```{r}
library(glmnet)
runGlmNet <- function(alpha) {
  set.seed(500)
  cv <- cv.glmnet(Boston.mat[train,],y.train,alpha=alpha)
  best.lam <- cv$lambda.min
  mod <- glmnet(Boston.mat[train,],y.train,alpha=alpha)
  pred <- predict(mod,newx=Boston.mat[test,],s=best.lam)
  err<-mean((pred-y.test)^2)
  list(mod=mod,cv=cv,best.lam=best.lam,err=err)
}
# RIDGE
ridge.mod<-runGlmNet(0)
ridge.mod$err
```
```{r}
#Lasso
lasso.mod<-runGlmNet(1)
lasso.mod$err
```
## PCR 13 components looks good
```{r}
library(pls)
set.seed(500)
pcr.mod <- pcr(crim~.,data=Boston,subset=train,scale=TRUE,validation="CV")
summary(pcr.mod)
```
```{r}
validationplot(pcr.mod,val.type="MSEP")
```
```{r}
pcr.pred <- predict(pcr.mod,Boston[test,],ncomp=9)
pcr.err <- mean((pcr.pred-y.test)^2)
```
## to summarize, ridge model performed the best with lowest MRSE, followed by Lasso, then a linear regression
## best subset did not improve on the full lm either using validation set approach
```{r}
lasso.err <- lasso.mod$err
ridge.err <- ridge.mod$err
rbind(lm.err, lm.full.err, ridge.err, lasso.err, pcr.err)
```
## Chapter 4 question 10
```{r}
package="ISLR"
data(Weekly,package="ISLR")
summary(Weekly)
```
```{r}
cor(Weekly[,-c(1,9)])
```
## all pairwise comparisons of the predictors with the color denoting the direction. almost no trend here
```{r}
pairs(Weekly[-(8:9)],col=Weekly$Direction)
```
## Plotting the predictors against the `Today` variable. no trend here
```{r}
par(mfcol=c(3,2))
predictors <- c(paste0("Lag",1:5),"Volume")
for (p in predictors){
  plot(Weekly[,p],Weekly[,"Today"],
       ylab="Today",
       xlab=p,
       col=Weekly[,"Direction"])
}
```
## 10(b)

```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,data=Weekly, family=binomial)
summary(glm.fit)
```
## only lag 2 is statistically significant, dont really do anything for us here. since we got 6 predictors
## 10(c)

```{r}
glm.probs <- predict(glm.fit,type="response")
glm.pred <- rep("Down",nrow(Weekly))
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred,Weekly$Direction) #confusion table
```
## the logistic regression is making false positive and false negative prediction a lot. almost a 50/50 flip coin.

## 10(d)
```{r}
train.inds <- Weekly$Year <= 2008
train.data <- Weekly[train.inds,]
test.data <- Weekly[!train.inds,]
test.n <- nrow(test.data)
lag2.mod <- glm(Direction ~ Lag2, data=Weekly, family=binomial, subset=train.inds)
summary(lag2.mod)
```

```{r}
lag2.pred <- rep("Down",test.n)
lag2.probs <- predict(glm.fit,test.data,type="response")
lag2.pred[lag2.probs>.5] <- "Up"
table(lag2.pred,test.data$Direction)
```
```{r}
sum(lag2.pred==test.data$Direction)/test.n
```
## over all correct prediction rate is 0.625. confusion matrix shows a quantifiable improvement.


## 10(g)
```{r}
library(class)
set.seed(1)
train.X <- as.matrix(train.data[,"Lag2"])
train.Y <- as.matrix(train.data[,"Direction"])
test.X <- as.matrix(test.data[,"Lag2"])
test.Y <- as.matrix(test.data[,"Direction"])
lag2.knn.pred <- knn(train.X,test.X,train.Y,k=1)
table(lag2.knn.pred,test.Y)
```
```{r}
sum(lag2.knn.pred == test.Y)/test.n
```
## K=1 is noisy, it is almost a flip coin. 

## 10(h)
## the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictorgives us the best prediction

## 10(i)
## let's play with KNN value

```{r}
k.vals <- 1:50
err <- double(50)
for (k in k.vals){
  train.X <- as.matrix(train.data[,"Lag2"])
  train.Y <- as.matrix(train.data[,"Direction"])
  test.X <- as.matrix(test.data[,"Lag2"])
  test.Y <- as.matrix(test.data[,"Direction"])
  lag2.knn.pred <- knn(train.X,test.X,train.Y,k=k)
  table(lag2.knn.pred,test.Y)
  err[k]<-sum(lag2.knn.pred == test.Y)/test.n
}
plot(k.vals,err,type="b")
```

## k=1 gives the best result on the test data.
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


##Chapter 10
##8(a)
```{r}
data(Carseats,package="ISLR")
set.seed(562)
train<-sample.int(nrow(Carseats),size=nrow(Carseats)/2)
```
## 8(b)
```{r}
library(tree)
tree.mod <- tree(Sales ~ ., data=Carseats, subset=train)
plot(tree.mod)
text(tree.mod,pretty=0)
```
## From the tree, shelf location is most important in determining sales, price is second most important

```{r}
y.pred<-predict(tree.mod, Carseats[-train,])
y.test<-Carseats[-train,]$Sales
mean((y.pred-y.test)^2)
```
## MSE is 5.091466


## 8(c)
```{r}
set.seed(562)
tree.mod.cv <- cv.tree(tree.mod)
tree.mod.cv
```

```{r}
plot(tree.mod.cv)
```

## size with the smallest deviance is 12.

```{r}
best.size <- tree.mod.cv$size[which.min(tree.mod.cv$dev)]
prune.mod <- prune.tree(tree.mod,best=best.size)
y.pred<-predict(prune.mod, Carseats[-train,])
mean((y.pred-y.test)^2)
```
## the prune test did not improve the MSE in this case

##8(d)

```{r}
library(randomForest)
set.seed(526)
bag.mod <- randomForest(Sales ~ ., data=Carseats, subset=train, mtry=(ncol(Carseats)-1), importance=TRUE)
y.pred<-predict(bag.mod,Carseats[-train,])
mean((y.pred-y.test)^2)
```
## MSE is improved by bagging method

```{r}
importance(bag.mod)
```
## price and shelf location are most important

```{r}
mtry<-1:10
errors <- (length(mtry))
set.seed(526)
for(i in seq_along(mtry)){
  m <- mtry[i]
  rf.mod <- randomForest(Sales ~ ., data=Carseats, subset=train, mtry=m)
  y.pred<-predict(rf.mod,Carseats[-train,])
  errors[i]<-mean((y.pred-y.test)^2)
}
plot(mtry,errors,type="b",pch="x")
points(mtry[which.min(errors)],min(errors),col="red",pch="x")
```

```{r}
errors
```


## Errors start to flat out at m=6 to 10. errors are 3.194614 3.195120 3.198307 3.237333 3.251480

## Chapter 8 question 11

##11(a)
```{r}
data(Caravan,package="ISLR")
train<-1:1000
```
##11(b)
```{r}
library(randomForest)
set.seed(526)
bag.mod <- randomForest(Purchase ~ ., data=Caravan,subset=train,mtry=ncol(Caravan)-1,shrinkage=0.01)
imp.vec<-importance(bag.mod)
head(imp.vec[order(imp.vec,decreasing=TRUE),])
```
## MGODGE PPERSAUT  MOSTYPE   MGODPR MOPLHOOG   PBRAND  are most important predictors

```{r}
y.test <- Caravan[-train,ncol(Caravan)]
y.pred <- ifelse(predict(bag.mod,Caravan[-train,],type="prob")[,"Yes"]>.2,"Yes","No")
table(y.pred,y.test)
```

## portion of ppl predicted to make a purchase actually makes a purchase is 11.67%
## knn=5
```{r}
library(class)
train.X <- Caravan[train,-ncol(Caravan)]
train.Y <- Caravan[train,ncol(Caravan)]
test.X <- Caravan[-train,-ncol(Caravan)]
knn.pred <- knn(train.X,test.X,train.Y,k=5)
table(knn.pred, y.test)

```
## knn=4 prediction sucess is still about 11.42%

##Off-Book problem 1
```{r}
rm(list=ls())
BeautyData= read.csv("C:/Users/Lai Jiang/Desktop/UT MSBA Summer Courses/Predictive Modeling/BeautyData.csv",header=T)
attach(BeautyData)
pairs(BeautyData)
```
## Beauty Score can be seen has some correlation with Course Evaluation. Let's evaluate the all variables except Course Evals and their effects on Course Evals.

```{r}
beauty.to.female = lm(BeautyScore~female)
summary(beauty.to.female)
```
## one DOES find a correlation between BeautyScore and female (significant p-value)
## on average, females tend to be ranked 0.2 higher on beautyScore, but adjusted r-squared suggest that beautyscore is explained in a very limited way by gender despite being statistically significant, 

```{r}
set.seed(1000)
BeautyData.mat <- model.matrix(CourseEvals ~ ., BeautyData)
train<-sample.int(nrow(BeautyData),size=nrow(BeautyData)*.5)
test<--train
y.train <- BeautyData$CourseEvals[train]
y.test <- BeautyData$CourseEvals[test]
```

## linear models, divide the data into test and sample, use best subsets to select the best train set using BIC
## then get the MSE on the test set
```{r}
require(leaps)
full.fit <- regsubsets(CourseEvals ~ ., data=BeautyData[train,],nvmax=ncol(BeautyData))
lm.best.coef <- coef(full.fit,id=which.min(summary(full.fit)$bic))
lm.pred <- BeautyData.mat[test,names(lm.best.coef)]%*%lm.best.coef
lm.err <- mean((lm.pred-y.test)^2)
lm.err
```
## MSE is 0.187, decent MSE
```{r}
reg.summary = summary(full.fit)
which.max(reg.summary$adjr2) #best model involves 5 predictors
coef(full.fit, 5)
```
## this linear regression from subsets selection shows that being 'attractive' gives the over rating a 0.30 increase per 1 pt in beautyscore. we can probably ignore the other data since it is more binary 1 or 0 for female, lower, non-english or tenuretrack.

## although the data suggest that professor being more attractive is correlated with higher course evaluation score, however, we cannot discern if the course evaluation score is higher because students really enjoy the course and learn with more passion give the professor is attractive or students penalize professors's course evaluation due to 'unattractiveness' of the professor despite the professor giving best effort to teach the students.



##Off-Book problem 2

```{r}
rm(list=ls())
MidCity= read.csv("C:/Users/Lai Jiang/Desktop/UT MSBA Summer Courses/Predictive Modeling/MidCity.csv",header=T)
attach(MidCity)
n = dim(MidCity)[1]
```
##2(1)
```{r}
BR = rep(0, n)
BR[Brick=="Yes"]=1
dn1 = rep(0,n) 
dn1[Nbhd==1]=1

dn2 = rep(0,n)
dn2[Nbhd==2]=1

MidCityModel = lm(Price~dn1+dn2+Offers+SqFt+Brick+Bedrooms+Bathrooms)
summary(MidCityModel)
```
## as we can see, there is a premius of 17297.35 dollars premium on brick houses. Adjusted R2 signifcant for this regression.



##2(2)
```{r}
dn3 = rep(0,n)
dn3[Nbhd==3]=1
MidCityModeldn3 = lm(Price~dn2+dn3+Offers+SqFt+Brick+Bedrooms+Bathrooms)
summary(MidCityModeldn3)
```
## the strong adjusted R squared and high dn3 (neighborhood3) coefficient shows that with all things considered equal, houes in dn3 will have a higher premium. it is logical since dn3 is a more modern, newer and more prestigious part of the town.

##2(3)
```{r}
MidCityModelbrickdn3 = lm(Price~dn2+Offers+SqFt+Brick*dn3+Bedrooms+Bathrooms)
summary(MidCityModelbrickdn3)
```
## the regression above introduces an interaction variable between neighborhood 3 and brick houses. the introduction variable reduces the premium for brick house from 17297.35 to 13826.47. This suggest that there is a less premium for brick houses in neighborhood 3 compared to the overall premium for a brick house.

##2(4)
```{r}
MidCityModelbrickdn1dn2= lm(Price~dn2+dn1+Offers+SqFt+Brick+Bedrooms+Bathrooms)
summary(MidCityModelbrickdn1dn2)
```

```{r}
#combining neighborhood1 and 2 into dn dummy variable col
dn = rep(0,n)
dn[Nbhd==1]=1
dn[Nbhd==2]=1
MidCityModelbrickdn= lm(Price~dn+Offers+SqFt+Brick+Bedrooms+Bathrooms)
summary(MidCityModelbrickdn)
```
## the regression on the right shows dn1 and dn2 as two separate dummie variables, the second regression shows dn1 and dn2 merged into dn as a single dummie variable. the coeff for dn1 and dn2 are very close with each other on first regression which suggest that they can be combied and still provide a good approximation. the results of the second regression further suggest this by showing dn coeff very close to dn1 and dn2 coeff in first regression. adjusted R square for both regression are very similar are 0.861 and 0.8616. both regression can explain approximately the same amount on the dependent variables. thus dn1 and dn2 can be combined


##Off-Book problem 3

##1. Different city has different level of police requirement and the reason for high police presence varies among cities. High crime city requires more cop and cops are there because of high crime. you cannot use high crime city to compare to low crime city.

##2. Researchers isolated the effect of more police on the street for reason unrelated to crime on street-crime. they ask when terrorism alert goes to orange, does the extra police presense decrease street crime. They also checked hypothesis of whether tourists were less likely to visit D.C due to orange terrorist alert level and found that ridership did not diminish so the numbder of victims though decreased by police presence but was not affected by ridership in D.C during orange alert day

##3. they control ridership to see if less tourists were likely to visit D.C. thus decrease the number of of potential victims to street crime and thus could potential cloud the effect of high police presence on street crime

##4 the model decribes the relation between high police presence not due to street crime on the street crime level. it showed that high police presence due to non-crime related reason does decrease the crime by -7.316 with coefficient standard error of 2.877. R2 is .14, so high police presence in this situation is weakly correlated reduction in crime.


##off-book problem 4
```{r}
library(MASS)
View(Boston)
attach(Boston)
```

#Summary of the data
```{r}
summary(Boston)
```
#Standardize the x's (the first 3 columns)

```{r}
Bostonsc = Boston #Create auxiliary copy of the matrix
```
```{r}
library(nnet)
set.seed(999)
```
```{r}
bnn = nnet(medv ~ crim, #Formula
           data = Bostonsc, #Data frame with the traning set
           size=3, #Units in the hidden layer
           decay=0.1, #Parameter for weight decay
           linout=T) #Linear output
```
```{r}
fbnn = predict(bnn,Bostonsc) #Gets the models fits for the data
plot(Bostonsc$crim,Bostonsc$medv) #Dispersion plot of food and price
oo = order(Bostonsc$crim) #Get the indices that will order the column food
lines(Bostonsc$crim[oo],fbnn[oo],col="red",lwd=2) #Line of the fits
abline(lm(medv~crim,Bostonsc)$coef) #Compare with the OLS fit

#What does this mean? Try to interpret looking at the Neural network
summary(bnn)
NeuralNetTools::plotnet(bnn)
```
```{r}
set.seed(999)
bnn = nnet(medv ~ crim, #Formula
           data = Bostonsc, #Data frame with the traning set
           size=5, #Units in the hidden layer
           decay=0.1, #Parameter for weight decay
           linout=T) #Linear output
print(summary(bnn))
```
## Size and Decay

#Let us modify the number of nodes in the hidden layer and decay values

#Four different fits
```{r}
set.seed(999)
bnn1 = nnet(medv ~ crim,Bostonsc,size=3,decay=.5,linout=T)
bnn2 = nnet(medv ~ crim,Bostonsc,size=3,decay=.00001,linout=T)
bnn3 = nnet(medv ~ crim,Bostonsc,size=50,decay=.5,linout=T)
bnn4 = nnet(medv ~ crim,Bostonsc,size=50,decay=.00001,linout=T)
```
```{r}
temp = data.frame(medv = Bostonsc$medv, crim = Bostonsc$crim) #The data
#The predictions of each model for the data
bnnf1 = predict(bnn1,temp)
bnnf2 = predict(bnn2,temp)
bnnf3 = predict(bnn3,temp)
bnnf4 = predict(bnn4,temp)
```
#Plotting the fits
```{r}
par(mfrow=c(2,2)) #Plot window: 2 row, 2 columns
```



```{r}
plot(Bostonsc$crim,Bostonsc$medv, xlab = "Crim", ylab = "Medv") #Scatterplot
lines(Bostonsc$crim[oo],bnnf1[oo],lwd=2) #Adding the lines of predicted values
title("size=3, decay=.5")

plot(Bostonsc$crim,Bostonsc$medv, xlab = "Crim", ylab = "Medv")
lines(Bostonsc$crim[oo],bnnf2[oo],lwd=2)
title("size=3, decay=.00001")

plot(Bostonsc$crim,Bostonsc$medv, xlab = "Crim", ylab = "Medv")
lines(Bostonsc$crim[oo],bnnf3[oo],lwd=2)
title("size = 50, decay = .5")

plot(Bostonsc$crim,Bostonsc$medv, xlab = "Crim", ylab = "Medv")
lines(Bostonsc$crim[oo],bnnf4[oo],lwd=2)
title("size = 50, decay = .00001")

```
##################################################
### Iterative Fitting and Random Starting Values

#You can control the number of iterations
```{r}
set.seed(999)
bnn3 = nnet(medv~crim,Bostonsc,size=50,decay=.5,linout=T)
bnn3 = nnet(medv~crim,Bostonsc,size=50,decay=.5,linout=T,maxit=20)
bnn3 = nnet(medv~crim,Bostonsc,size=50,decay=.5,linout=T,maxit=1000)
```


#Run one of the models above and then print the result
```{r}
bnnf3 = predict(bnn3,temp) #Predicted values for the model
par(mfrow=c(1,1)) #Plot window: 1 row, 1 column
plot(Bostonsc$crim,Bostonsc$medv,
     xlab = "crime", ylab = "medv")
lines(Bostonsc$crim[oo],bnnf3[oo],lwd=2)
```
```{r}
set.seed(999)
temp = nnet(medv~crim,Bostonsc,size=2,decay=.001)
summary(temp) #See the weights
```


##PM problem 5 Final Project
##For this project with my group. I was first to start a group chat to help all the other members in our group to stay engaged and we carefully selected data together and I was able to analyze the difficulty of the project and what conclusion we are trying to draw from the data.
##Once we got the wine set data, I help with data selection as well as making sure all the members are ok with the models we will use to present for our project. I also coordinated subsequent meetings with members of our group to ensure everyone is aware of the progress we are making and understand the data analysis and the meaning of the result.
##Finally, I helped on the PowerPoint design recommendation to make our data presentable.
